# -*- coding: utf-8 -*-
"""HM5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZK62ykI-y4PuN_00FD3EIDYWctcwctv-
"""

import os
from multiprocessing import freeze_support
from sam import SAM
import torch
from torchvision.datasets import CIFAR10
from torchvision.transforms import v2
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
from torch.utils.tensorboard import SummaryWriter
import wandb

def get_default_device():
    if torch.cuda.is_available():
        return torch.device('cuda')
        # For multi-gpu workstations, PyTorch will use the first available GPU (cuda:0), unless specified otherwise
        # (cuda:1).
    if torch.backends.mps.is_available():
        return torch.device('mos')
    return torch.device('cpu')


class CachedDataset(Dataset):
    def __init__(self, dataset, cache=True):
        if cache:
            dataset = tuple([x for x in dataset])
        self.dataset = dataset

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, i):
        return self.dataset[i]


class MLP(torch.nn.Module):
    def __init__(self, input_size, hidden_size_1, hidden_size_2,hidden_size_3, output_size):
        super(MLP, self).__init__()
        self.fc1 = torch.nn.Linear(input_size, hidden_size_1)
        self.fc2 = torch.nn.Linear(hidden_size_1, hidden_size_2)
        self.fc3 = torch.nn.Linear(hidden_size_2, hidden_size_3)
        self.fc4 = torch.nn.Linear(hidden_size_3, output_size)
        self.relu = torch.nn.ReLU(inplace=True)

    def forward(self, x):
        return self.fc4(self.relu(self.fc3(self.relu(self.fc2(self.relu(self.fc1(x)))))))
        #return self.fc2(self.relu(self.fc1(x)))
        # x = self.fc1(x)
        # x = self.relu(x)
        # x = self.fc2(x)
        # return x


def accuracy(output, labels):
    fp_plus_fn = torch.logical_not(output == labels).sum().item()
    all_elements = len(output)
    return (all_elements - fp_plus_fn) / all_elements


def train(model, train_loader, criterion, optimizer,writer,epoch, device):
    model.train()
    epoch_loss = 0.0
    epoch_train_loss = 0.0
    all_outputs = []
    all_labels = []

    for batch_idx, (data, labels) in enumerate(train_loader):
        data = data.to(device, non_blocking=True)
        labels = labels.to(device, non_blocking=True)

        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, labels)

        loss.backward()

        # torch.nn.utils.clip_grad_norm_(model.parameters(), 5)

        optimizer.step()
        optimizer.zero_grad(set_to_none=True)


        epoch_train_loss += loss.item()  # Accumulate the batch loss

        writer.add_scalar("Train/BatchLoss", loss.item(), epoch * len(train_loader) + batch_idx)
        output = output.softmax(dim=1).detach().cpu().squeeze()
        labels = labels.cpu().squeeze()
        epoch_loss += loss.item()
        all_outputs.append(output)
        all_labels.append(labels)
        optimizer.step()

    all_outputs = torch.cat(all_outputs).argmax(dim=1)
    all_labels = torch.cat(all_labels)

    epoch_train_loss /= len(train_loader.dataset)
    return round(accuracy(all_outputs, all_labels), 4), epoch_train_loss


def val(model, val_loader,criterion, device):
    model.eval()
    all_outputs = []
    all_labels = []
    epoch_loss = 0.0
    for data, labels in val_loader:
        data = data.to(device, non_blocking=True)

        with torch.no_grad():
            output = model(data)
        loss = criterion(output, labels)
        epoch_loss += loss.item()
        output = output.softmax(dim=1).cpu().squeeze()
        labels = labels.squeeze()
        all_outputs.append(output)
        all_labels.append(labels)

    all_outputs = torch.cat(all_outputs).argmax(dim=1)
    all_labels = torch.cat(all_labels)
    val_loss=epoch_loss / len(val_loader.dataset)
    return round(accuracy(all_outputs, all_labels), 4),val_loss


def do_epoch(model, train_loader, val_loader, criterion, optimizer,writer,epoch, device):
    acc, train_loss = train(model, train_loader, criterion, optimizer,writer,epoch, device)
    acc_val,val_loss = val(model, val_loader,criterion, device)
    # torch.cuda.empty_cache()
    return acc,train_loss, acc_val,val_loss


def get_model_norm(model):
    norm = 0.0
    for param in model.parameters():
        norm += torch.norm(param)
    return norm


def main_exec(device=get_default_device()):
    transforms = [
        v2.ToImage(),
        v2.ToDtype(torch.float32, scale=True),
        v2.Resize((28, 28), antialias=True),
        v2.Grayscale(),
        torch.flatten,
    ]
    run = wandb.init(
        project="HM5",
        notes="test",
    )
    device=get_default_device()
    learning_rate = wandb.config.learning_rate
    batch_size = wandb.config.batch_size
    epochs = wandb.config.num_epochs
    optimizer_type = wandb.config.optimizer
    data_path = '../data'
    train_dataset = CIFAR10(root=data_path, train=True, transform=v2.Compose(transforms), download=True)
    val_dataset = CIFAR10(root=data_path, train=False, transform=v2.Compose(transforms), download=True)
    train_dataset = CachedDataset(train_dataset)
    val_dataset = CachedDataset(val_dataset)

    sharpness = 0.1
    base_learning_rate = learning_rate
    weight_decay = 0.001

    model = MLP(784, 100, 500, 250, 10)
    model = model.to(device)

    if optimizer_type == 'Adam':
        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)
    elif optimizer_type == 'SGD':
        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.99, weight_decay=3e-05, nesterov=True)
    elif optimizer_type == 'RMSprop':
        optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate, alpha=0.9)
    elif optimizer_type == 'Adagrad':
        optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)
    else:
        base_optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.99, weight_decay=3e-05, nesterov=True)
        optimizer = SAM(model.parameters(), base_optimizer, rho=0.05)
    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    criterion = torch.nn.CrossEntropyLoss()
    # epochs = 50

    # batch_size = 256
    val_batch_size = 500
    num_workers = 2
    persistent_workers = (num_workers != 0)
    pin_memory = device.type == 'cuda'
    train_loader = DataLoader(train_dataset, shuffle=True, pin_memory=pin_memory, num_workers=num_workers,
                              batch_size=batch_size, drop_last=True, persistent_workers=persistent_workers)
    val_loader = DataLoader(val_dataset, shuffle=False, pin_memory=True, num_workers=0, batch_size=val_batch_size,
                            drop_last=False)


    writer = SummaryWriter()
    writer.add_text("Config", f"Learning Rate: {optimizer.param_groups[0]['lr']}")
    writer.add_text("Config", f"Batch Size: {batch_size}")
    writer.add_text("Config", f"Optimizer: {type(optimizer).__name__}")
    writer.add_text("Config", f"Number of Epochs: {epochs}")

    tbar = tqdm(tuple(range(epochs)))
    for epoch in tbar:
        acc,train_loss, acc_val,val_loss = do_epoch(model, train_loader, val_loader, criterion, optimizer,writer,epoch, device)
        tbar.set_postfix_str(f"Acc: {acc}, Acc_val: {acc_val}")
        writer.add_scalar("Train/Accuracy", acc, epoch)
        writer.add_scalar("Train/EpochLoss", train_loss, epoch)
        writer.add_scalar("Val/Accuracy", acc_val, epoch)
        writer.add_scalar("Val/EpochLoss", val_loss, epoch)
        writer.add_scalar("Model/Norm", get_model_norm(model), epoch)

def main(device=get_default_device()):

    sweep_config = {
      'method': 'random',
      'project': 'HM5',
      'parameters': {
          'learning_rate': {'values': [0.001,0.05, 0.01, 0.1]},
          'batch_size': {'values': [32, 64, 128]},
          'num_epochs': {'values': [50, 100, 150]},
          'optimizer': {'values': ['SGD','Adam','RMSprop','Adagrad']}
      }
    }

    sweep_id = wandb.sweep(sweep_config, project='HM5')
    wandb.agent(sweep_id, function=main_exec)

if __name__ == '__main__':
    freeze_support()
    main()