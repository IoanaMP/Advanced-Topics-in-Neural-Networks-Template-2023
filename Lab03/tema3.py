# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CLq5KTG-NlkC6SCN4uYHzDnNvXIeQLBa
"""

import torch
import torchvision
from torchvision import transforms
from torch.utils.data import DataLoader
import torch.nn as nn


def one_hot_encode(labels, num_classes):
    encoded_labels = torch.zeros(len(labels), num_classes)
    for i, label in enumerate(labels):
        encoded_labels[i][label] = 1
    return encoded_labels

def collate(batch):
    images, labels = zip(*batch)
    encoded_labels = one_hot_encode(labels, num_classes=10)
    return torch.stack(images), encoded_labels

def custom_data_loader(root, train=True, batch_size=100, shuffle=True, pin_memory=True):
    transform = transforms.Compose([transforms.ToTensor()])
    mnist = torchvision.datasets.MNIST(root=root, train=train, transform=transform, download=True)
    data_loader = DataLoader(mnist, batch_size=batch_size, shuffle=shuffle, pin_memory=pin_memory, collate_fn=collate)
    return data_loader

def load_test_data(root, batch_size, shuffle, pin_memory):
    transform = transforms.Compose([transforms.ToTensor()])
    test_data = torchvision.datasets.MNIST(root=root, train=False, transform=transform, download=True)
    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=shuffle, pin_memory=pin_memory)
    return test_loader

def sigmoid(x):
    return 1 / (1 + torch.exp(-x))

def forward_prop(X, W1, b1, W2, b2):
    z1 = torch.matmul(X, W1) + b1
    h1 = sigmoid(z1)
    z2 = torch.matmul(h1, W2) + b2
    y_pred = torch.softmax(z2, dim=1)
    return y_pred, z1, h1, z2

def backward(X, W1, b1, W2, b2, y_true, mu, z1, h1, z2):
    y_pred, z1, h1, z2 = forward_prop(X, W1, b1, W2, b2)
    error = y_true - y_pred

    dW2 = torch.matmul(h1.t(), error)
    db2 = error.mean(axis=0)

    delta = torch.matmul(error, W2.t()) * h1 * (1 - h1)
    dW1 = torch.matmul(X.t(), delta)
    db1 = delta.mean(axis=0)

    W1 -= mu * dW1
    b1 -= mu * db1
    W2 -= mu * dW2
    b2 -= mu * db2

    return W1, b1, W2, b2

def train_model(train_loader, num_epochs, mu, device):
    input_size = 784
    hidden_size = 100
    output_size = 10
    W1 = torch.empty(input_size, hidden_size, device=device).normal_(mean=0, std=1)
    b1 = torch.empty(1, hidden_size, device=device).zero_()
    W2 = torch.empty(hidden_size, output_size, device=device).normal_(mean=0, std=1)
    b2 = torch.empty(1, output_size, device=device).zero_()

    for epoch in range(num_epochs):

        correct = 0
        total = 0
        loss = 0;
        for inputs, labels in train_loader:
            inputs = inputs.view(inputs.size(0), -1).to(device)
            labels = labels.to(device)

            y_true = labels

            y_pred, z1, h1, z2 = forward_prop(inputs, W1, b1, W2, b2)
            error = y_true - y_pred

            dW2 = torch.matmul(h1.t(), error)
            db2 = error.mean(axis=0)

            delta = torch.matmul(error, W2.t()) * h1 * (1 - h1)
            dW1 = torch.matmul(inputs.t(), delta)
            db1 = delta.mean(axis=0)

            W1 -= mu * dW1
            b1 -= mu * db1
            W2 -= mu * dW2
            b2 -= mu * db2

            loss += torch.nn.functional.cross_entropy(y_pred, y_true).item()

            _, predicted = torch.max(y_pred, 1)
            _, true_labels = torch.max(y_true, 1)
            total += true_labels.size(0)
            correct += (predicted == true_labels).sum().item()

        accuracy = correct / total * 100
        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss / total}, Accuracy: {accuracy}%')
        return W1,b1,W2,b2

def one_hot_to_labels(one_hot_labels):
    return torch.argmax(one_hot_labels, dim=1)

def test_model(test_loader, w_hidden_layer, b_hidden_layer, w_last_layer, b_last_layer):
    device = w_hidden_layer.device

    correct = 0
    total = 0

    for inputs, labels in test_loader:
        inputs = inputs.view(inputs.size(0), -1).to(device)
        y_true = labels.to(device)

        y_pred, _, _, _ = forward_prop(inputs, w_hidden_layer, b_hidden_layer, w_last_layer, b_last_layer)

        predicted = torch.max(y_pred, 1).indices

        true_labels = one_hot_to_labels(y_true)

        total += true_labels.size(0)
        correct += (predicted == true_labels).sum().item()

    accuracy = (correct / total) * 100
    print(f'Test Accuracy: {accuracy:.2f}%')

if __name__ == '__main__':
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    train_loader = custom_data_loader('./data', train=True, batch_size=100, shuffle=True, pin_memory=True)

    num_epochs = 10
    learning_rate = 0.01
    test_loader = custom_data_loader('./data', train=False, batch_size=100, shuffle=False, pin_memory=True)

    W1, b1, W2, b2 = train_model(train_loader, num_epochs, learning_rate, device)
    test_model(test_loader, W1, b1, W2, b2)